import json
import time
import random
import math
from zai import ZhipuAiClient
import os
import pickle
import numpy as np
import os
from attention_utils import compute_traj_attention, build_attn_dsl_block
from difflib import SequenceMatcher
import re

os.environ["http_proxy"] = "http://localhost:7890"
os.environ["https_proxy"] = "http://localhost:7890"
# å®šä¹‰é—®é¢˜
DEBATE_QUESTION = "å¦‚ä½•è¯„ä»·æ— äººæœºé›†ç¾¤çš„é£è¡Œè½¨è¿¹ä¼˜åŒ–å’ŒååŒä½œä¸šèƒ½åŠ›ï¼Ÿ"

# ========== åˆ›æ–°æœºåˆ¶é…ç½® ==========
DEBATE_CONFIG = {
    "hierarchical": {
        "enabled": True,
        "complexity_thresholds": {"low": 0.3, "high": 0.7},
        "max_rounds": {"layer1": 1, "layer2": 2, "layer3": 3}
    },
    "adversarial": {
        "enabled": True,
        "red_team_ratio": 0.4  # 40%çš„agentä½œä¸ºçº¢é˜Ÿ
    },
    "role_rotation": {
        "enabled": True,
        "rotation_mode": "adaptive"  # "round_robin" or "adaptive"
    },
    "evidence_chain": {
        "enabled": True,
        "min_evidence_count": 3,
        "require_data_references": True
    },
    "consensus_modeling": {
        "enabled": True,
        "convergence_threshold": 0.92,
        "dimensions": ["score", "semantic", "priority", "concern"]
    },
    "meta_cognitive": {
        "enabled": True,
        "quality_threshold": 0.7,
        "intervention_enabled": True
    }
}

# è¯„åˆ†æ ‡å‡†é…ç½®
SCORING_CRITERIA = {
    "flight_control": {
        "name": "è½¨è¿¹è¯„ä¼°è¯„åˆ†",
        "weight": 0.0,
        "disabled": True,
        "metrics": {
            "heading_smoothness": {"weight": 0.35, "name": "èˆªå‘å¹³æ»‘åº¦"},
            "path_efficiency": {"weight": 0.30, "name": "è·¯å¾„æ•ˆç‡"},
            "sharp_turn_ratio": {"weight": 0.20, "name": "æ€¥è½¬æ¯”ä¾‹(è¶Šä½è¶Šå¥½)"},
            "altitude_variation": {"weight": 0.15, "name": "é«˜åº¦æ³¢åŠ¨(è¶Šä½è¶Šå¥½)"}
        }
    },
    "swarm_coordination": {
        "name": "å†³ç­–ä¸ååŒè¯„åˆ†",
        "weight": 0.4,
        "metrics": {
            "formation_stability": {"weight": 0.3, "name": "ç¼–é˜Ÿç¨³å®šæ€§"},
            "communication_quality": {"weight": 0.25, "name": "é€šä¿¡è´¨é‡"},
            "coordination_delay": {"weight": 0.25, "name": "åè°ƒå»¶è¿Ÿ"},
            "task_completion": {"weight": 0.2, "name": "ä»»åŠ¡å®Œæˆåº¦"}
        }
    },
    "safety_assessment": {
        "name": "å®‰å…¨è¯„ä¼°è¯„åˆ†",
        "weight": 0.0,
        "metrics": {
            "collision_avoidance": {"weight": 0.4, "name": "é¿ç¢°èƒ½åŠ›"},
            "emergency_response": {"weight": 0.3, "name": "åº”æ€¥å“åº”"},
            "risk_management": {"weight": 0.3, "name": "é£é™©ç®¡æ§"}
        }
    }
}

# è¾…åŠ©ï¼šæ–¹å‘å­—ç¬¦ä¸²
COMPASS_DIRS = [
    ("N", 0), ("NNE", 22.5), ("NE", 45), ("ENE", 67.5),
    ("E", 90), ("ESE", 112.5), ("SE", 135), ("SSE", 157.5),
    ("S", 180), ("SSW", 202.5), ("SW", 225), ("WSW", 247.5),
    ("W", 270), ("WNW", 292.5), ("NW", 315), ("NNW", 337.5)
]


# è‡ªåŠ¨è¯„åˆ†ç®—æ³•
def calculate_flight_scores(flight_data):
    """
    åŸºäºé£è¡Œæ•°æ®è®¡ç®—å„é¡¹è¯„åˆ†
    """
    scores = {}
    
    # 1. é£è¡Œæ§åˆ¶è¯„åˆ†
    flight_control_scores = {}
    
    # è½¨è¿¹å¹³æ»‘åº¦è¯„åˆ† (åŸºäºé€Ÿåº¦å˜åŒ–å’Œèˆªå‘å˜åŒ–)
    avg_speed_variance = sum([
        sum([abs(p["speed"] - drone["performance_metrics"]["avg_speed"]) 
             for p in drone["trajectory"]]) / len(drone["trajectory"])
        for drone in flight_data["drones"]
    ]) / len(flight_data["drones"])
    trajectory_smoothness = max(0, 100 - avg_speed_variance * 5)
    flight_control_scores["trajectory_smoothness"] = min(100, trajectory_smoothness)
    
    # é«˜åº¦ç¨³å®šæ€§è¯„åˆ†
    altitude_variances = []
    for drone in flight_data["drones"]:
        altitudes = [p["altitude"] for p in drone["trajectory"]]
        avg_alt = sum(altitudes) / len(altitudes)
        variance = sum([(alt - avg_alt) ** 2 for alt in altitudes]) / len(altitudes)
        altitude_variances.append(variance)
    avg_altitude_variance = sum(altitude_variances) / len(altitude_variances)
    altitude_stability = max(0, 100 - avg_altitude_variance / 10)
    flight_control_scores["altitude_stability"] = min(100, altitude_stability)
    
    # é€Ÿåº¦ä¸€è‡´æ€§è¯„åˆ†
    speed_consistency = max(0, 100 - avg_speed_variance * 3)
    flight_control_scores["speed_consistency"] = min(100, speed_consistency)
    
    # èƒ½æºæ•ˆç‡è¯„åˆ† (åŸºäºç”µæ± æ¶ˆè€—)
    avg_battery_consumption = sum([
        drone["performance_metrics"]["battery_consumption"] 
        for drone in flight_data["drones"]
    ]) / len(flight_data["drones"])
    energy_efficiency = max(0, 200 - avg_battery_consumption * 2)
    flight_control_scores["energy_efficiency"] = min(100, energy_efficiency)
    
    scores["flight_control"] = flight_control_scores
    
    # 2. é›†ç¾¤ååŒè¯„åˆ†
    swarm_coordination_scores = {}
    
    # ç¼–é˜Ÿç¨³å®šæ€§è¯„åˆ†
    formation_stability = flight_data["swarm_metrics"]["formation_stability"]
    swarm_coordination_scores["formation_stability"] = formation_stability
    
    # é€šä¿¡è´¨é‡è¯„åˆ†
    communication_quality = flight_data["swarm_metrics"]["communication_success_rate"]
    swarm_coordination_scores["communication_quality"] = communication_quality
    
    # åè°ƒå»¶è¿Ÿè¯„åˆ† (å»¶è¿Ÿè¶Šä½åˆ†æ•°è¶Šé«˜)
    coordination_delay = flight_data["swarm_metrics"]["coordination_delay_avg"]
    coordination_delay_score = max(0, 100 - coordination_delay / 2)
    swarm_coordination_scores["coordination_delay"] = coordination_delay_score
    
    # ä»»åŠ¡å®Œæˆåº¦è¯„åˆ†
    task_completion = flight_data["swarm_metrics"]["task_completion_rate"]
    swarm_coordination_scores["task_completion"] = task_completion

    # å•æœºä»»åŠ¡ï¼šååŒè¯„åˆ†ä¸é€‚ç”¨ï¼Œç½®é›¶å¹¶æ ‡è®°
    if int(flight_data.get("drone_count", 1)) <= 1:
        swarm_coordination_scores["formation_stability"] = 0.0
        swarm_coordination_scores["communication_quality"] = 0.0
        swarm_coordination_scores["coordination_delay"] = 0.0
        swarm_coordination_scores["task_completion"] = 0.0
        swarm_coordination_scores["not_applicable"] = True
    
    scores["swarm_coordination"] = swarm_coordination_scores
    
    # 3. å®‰å…¨è¯„ä¼°è¯„åˆ†
    safety_assessment_scores = {}
    
    # é¿ç¢°èƒ½åŠ›è¯„åˆ† (åŸºäºé¿ç¢°äº‹ä»¶æ•°é‡)
    collision_events = flight_data["swarm_metrics"]["collision_avoidance_events"]
    collision_avoidance = max(0, 100 - collision_events * 15)
    safety_assessment_scores["collision_avoidance"] = collision_avoidance
    
    # åº”æ€¥å“åº”è¯„åˆ† (åŸºäºå¼‚å¸¸å¤„ç†)
    total_anomalies = sum([len(drone["anomalies"]) for drone in flight_data["drones"]])
    emergency_response = max(0, 100 - total_anomalies * 10)
    safety_assessment_scores["emergency_response"] = emergency_response
    
    # é£é™©ç®¡æ§è¯„åˆ† (ç»¼åˆè¯„ä¼°)
    risk_factors = collision_events + total_anomalies
    risk_management = max(0, 100 - risk_factors * 8)
    safety_assessment_scores["risk_management"] = risk_management
    
    scores["safety_assessment"] = safety_assessment_scores
    
    return scores


def calculate_weighted_scores(scores):
    """
    è®¡ç®—åŠ æƒç»¼åˆè¯„åˆ†
    """
    weighted_scores = {}
    total_score = 0.0
    
    for category, category_data in SCORING_CRITERIA.items():
        # è·³è¿‡å·²ç¦ç”¨çš„ç±»åˆ«ï¼ˆä¾‹å¦‚è½¨è¿¹è¯„ä¼°ï¼‰
        if category_data.get("disabled", False):
            continue
        category_score = 0.0
        category_weighted_score = 0.0
 
         # ååŒè¯„åˆ†åœ¨å•æœºä»»åŠ¡ä¸‹ä¸é€‚ç”¨
        not_applicable = bool(scores.get(category, {}).get("not_applicable", False))
         
        if not not_applicable:
            for metric, metric_data in category_data["metrics"].items():
                metric_score = scores[category][metric]
                weighted_metric_score = metric_score * metric_data["weight"]
                category_score += weighted_metric_score
             
            category_weighted_score = category_score * category_data["weight"]
            total_score += category_weighted_score
 
        weighted_scores[category] = {
            "score": category_score,
            "weighted_score": category_weighted_score,
            "details": scores[category],
            "not_applicable": not_applicable
        }
     
    weighted_scores["total_score"] = total_score
    return weighted_scores


def generate_expert_scoring(agent_name, flight_data, scores):
    """
    ä¸ºä¸“å®¶ç”Ÿæˆä¸“ä¸šè¯„åˆ†å’Œåˆ†æ
    """
    agent_info = AGENTS[agent_name]
    expert_scoring = {
        "expert": agent_name,
        "expertise": agent_info["expertise"],
        "focused_metrics": {},
        "professional_analysis": "",
        "recommendations": []
    }
    
    # æå–è¯¥ä¸“å®¶å…³æ³¨çš„è¯„åˆ†æŒ‡æ ‡
    for category in ["flight_control", "swarm_coordination", "safety_assessment"]:
        if category == "swarm_coordination" and int(flight_data.get("drone_count", 1)) <= 1:
            continue  # å•æœºä»»åŠ¡ï¼šååŒè¯„åˆ†ä¸é€‚ç”¨
        if category in scores:
            for metric in agent_info["scoring_focus"]:
                if metric in scores[category]:
                    expert_scoring["focused_metrics"][metric] = scores[category][metric]
    
    # è®¡ç®—ä¸“å®¶ä¸“ä¸šè¯„åˆ† (åŸºäºå…³æ³¨æŒ‡æ ‡çš„åŠ æƒå¹³å‡)
    if expert_scoring["focused_metrics"]:
        expert_score = sum(expert_scoring["focused_metrics"].values()) / len(expert_scoring["focused_metrics"])
        expert_scoring["expert_score"] = round(expert_score, 2)
    else:
        expert_scoring["expert_score"] = None
        expert_scoring["note"] = "å•æœºä»»åŠ¡ï¼ŒååŒè¯„åˆ†ä¸é€‚ç”¨"
    
    return expert_scoring


def generate_scoring_report(flight_data, scores, weighted_scores, expert_scorings):
    """
    ç”Ÿæˆç»¼åˆè¯„åˆ†æŠ¥å‘Š
    """
    report = {
        "evaluation_summary": {
            "total_score": round(weighted_scores["total_score"], 2),
            "grade": get_performance_grade(weighted_scores["total_score"]),
            "evaluation_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "drone_count": len(flight_data["drones"]),
            "mission_duration": flight_data["mission_info"]["duration_minutes"]
        },
        "category_scores": {},
        "expert_evaluations": expert_scorings,
        "detailed_metrics": scores,
        "recommendations": generate_recommendations(weighted_scores, expert_scorings),
        "performance_analysis": generate_performance_analysis(weighted_scores)
    }
    
    # åˆ†ç±»è¯„åˆ†è¯¦æƒ…
    for category, data in weighted_scores.items():
        if category != "total_score":
            grade = get_performance_grade(data["score"])
            note = ""
            if data.get("not_applicable"):
                grade = "N/A"
                note = "å•æœºä»»åŠ¡ï¼ŒååŒè¯„åˆ†ä¸é€‚ç”¨"
            report["category_scores"][category] = {
                "name": SCORING_CRITERIA[category]["name"],
                "score": round(data["score"], 2),
                "weighted_score": round(data["weighted_score"], 2),
                "weight": SCORING_CRITERIA[category]["weight"],
                "grade": grade,
                "note": note
            }
    
    return report


def get_performance_grade(score):
    """
    æ ¹æ®åˆ†æ•°è·å–ç­‰çº§
    """
    if score >= 90:
        return "ä¼˜ç§€"
    elif score >= 80:
        return "è‰¯å¥½"
    elif score >= 70:
        return "ä¸­ç­‰"
    elif score >= 60:
        return "åŠæ ¼"
    else:
        return "ä¸åŠæ ¼"


def generate_recommendations(weighted_scores, expert_scorings):
    """
    ç”Ÿæˆæ”¹è¿›å»ºè®®
    """
    recommendations = []
    
    # åŸºäºåˆ†æ•°ç”Ÿæˆå»ºè®®
    for category, data in weighted_scores.items():
        if category != "total_score" and data["score"] < 80:
            if data.get("not_applicable"):
                continue
            category_name = SCORING_CRITERIA[category]["name"]
            if data["score"] < 60:
                recommendations.append(f"{category_name}è¡¨ç°ä¸ä½³ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")
            elif data["score"] < 80:
                recommendations.append(f"{category_name}æœ‰æå‡ç©ºé—´ï¼Œå»ºè®®ä¼˜åŒ–")
    
    # åŸºäºä¸“å®¶è¯„åˆ†ç”Ÿæˆå»ºè®®
    for expert_scoring in expert_scorings:
        if expert_scoring.get("expert_score") is not None and expert_scoring["expert_score"] < 75:
            recommendations.append(f"å»ºè®®é‡ç‚¹å…³æ³¨{expert_scoring['expert']}æå‡ºçš„ä¸“ä¸šå»ºè®®")
    
    return recommendations


def generate_performance_analysis(weighted_scores):
    """
    ç”Ÿæˆæ€§èƒ½åˆ†æ
    """
    analysis = {
        "strengths": [],
        "weaknesses": [],
        "overall_assessment": ""
    }
    
    # åˆ†æä¼˜åŠ¿å’ŒåŠ£åŠ¿
    for category, data in weighted_scores.items():
        if category != "total_score":
            category_name = SCORING_CRITERIA[category]["name"]
            if data["score"] >= 85:
                analysis["strengths"].append(f"{category_name}è¡¨ç°ä¼˜ç§€")
            elif data["score"] < 70:
                analysis["weaknesses"].append(f"{category_name}éœ€è¦æ”¹è¿›")
    
    # æ€»ä½“è¯„ä¼°
    total_score = weighted_scores["total_score"]
    if total_score >= 85:
        analysis["overall_assessment"] = "æ— äººæœºé›†ç¾¤æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼Œå„é¡¹æŒ‡æ ‡å‡è¾¾åˆ°è¾ƒé«˜æ°´å¹³"
    elif total_score >= 75:
        analysis["overall_assessment"] = "æ— äººæœºé›†ç¾¤è¡¨ç°è‰¯å¥½ï¼Œéƒ¨åˆ†æŒ‡æ ‡ä»æœ‰ä¼˜åŒ–ç©ºé—´"
    elif total_score >= 65:
        analysis["overall_assessment"] = "æ— äººæœºé›†ç¾¤è¡¨ç°ä¸­ç­‰ï¼Œéœ€è¦åœ¨å¤šä¸ªæ–¹é¢è¿›è¡Œæ”¹è¿›"
    else:
        analysis["overall_assessment"] = "æ— äººæœºé›†ç¾¤è¡¨ç°ä¸ä½³ï¼Œéœ€è¦å…¨é¢ä¼˜åŒ–å’Œæ”¹è¿›"
    
    return analysis


# ========== åˆ›æ–°æœºåˆ¶1: åˆ†å±‚è¾©è®ºç»“æ„ ==========
def assess_issue_complexity(flight_data, scores, weighted_scores):
    """
    è¯„ä¼°é—®é¢˜å¤æ‚åº¦ï¼Œè¿”å›0-1ä¹‹é—´çš„å€¼
    """
    complexity_factors = []
    
    # å› ç´ 1: æ•°æ®æ–¹å·®
    try:
        drone_count = int(flight_data.get("drone_count", 1))
        if drone_count > 1:
            formation_std = abs(scores.get("swarm_coordination", {}).get("formation_stability", 50) - 50) / 50
            complexity_factors.append(formation_std)
    except:
        pass
    
    # å› ç´ 2: è¯„åˆ†åˆ†æ­§åº¦
    score_variance = 0.0
    category_scores = []
    for cat in ["flight_control", "swarm_coordination", "safety_assessment"]:
        if cat in scores and not scores[cat].get("not_applicable"):
            cat_score = weighted_scores.get(cat, {}).get("score", 0)
            category_scores.append(cat_score)
    if len(category_scores) >= 2:
        score_variance = np.std(category_scores) / 100.0
        complexity_factors.append(score_variance)
    
    # å› ç´ 3: å®‰å…¨å…³é”®æ€§
    safety_score = weighted_scores.get("safety_assessment", {}).get("score", 100)
    if safety_score < 70:
        complexity_factors.append(0.8)  # å®‰å…¨åˆ†æ•°ä½åˆ™å¤æ‚åº¦é«˜
    
    complexity = np.mean(complexity_factors) if complexity_factors else 0.5
    return min(1.0, max(0.0, complexity))

def route_to_debate_layer(complexity):
    """æ ¹æ®å¤æ‚åº¦è·¯ç”±åˆ°ç›¸åº”çš„è¾©è®ºå±‚"""
    thresholds = DEBATE_CONFIG["hierarchical"]["complexity_thresholds"]
    if complexity < thresholds["low"]:
        return "layer1"  # å¿«é€Ÿå…±è¯†
    elif complexity < thresholds["high"]:
        return "layer2"  # æ·±åº¦åˆ†æ
    else:
        return "layer3"  # å…ƒè¾©è®º

# ========== åˆ›æ–°æœºåˆ¶2: å¯¹æŠ—-åä½œæ··åˆåè®® ==========
def assign_red_blue_teams(agent_names, round_idx):
    """
    åˆ†é…çº¢è“é˜Ÿè§’è‰²
    çº¢é˜Ÿï¼šæ‰¹åˆ¤æ€§åˆ†æï¼Œå¯»æ‰¾é—®é¢˜
    è“é˜Ÿï¼šå»ºè®¾æ€§åˆ†æï¼Œæä¾›æ–¹æ¡ˆ
    """
    n_agents = len(agent_names)
    n_red = max(1, int(n_agents * DEBATE_CONFIG["adversarial"]["red_team_ratio"]))
    
    # è½®æ¢çº¢è“é˜Ÿæˆå‘˜
    red_indices = [(i + round_idx) % n_agents for i in range(n_red)]
    blue_indices = [i for i in range(n_agents) if i not in red_indices]
    
    team_assignments = {}
    for i, name in enumerate(agent_names):
        team_assignments[name] = "red" if i in red_indices else "blue"
    
    return team_assignments

def get_team_specific_prompt(team, base_prompt):
    """æ ¹æ®çº¢è“é˜Ÿè§’è‰²è°ƒæ•´æç¤º"""
    if team == "red":
        prefix = """
ã€çº¢é˜Ÿè§’è‰² - æ‰¹åˆ¤æ€§åˆ†æã€‘
ä½ çš„ä»»åŠ¡æ˜¯ä»¥æ‰¹åˆ¤æ€§æ€ç»´å®¡æŸ¥è¯„ä¼°ç»“æœï¼š
1. ä¸»åŠ¨å¯»æ‰¾æ•°æ®ä¸­çš„å¼‚å¸¸å’Œæ½œåœ¨é—®é¢˜
2. è´¨ç–‘è¿‡äºä¹è§‚çš„è¯„åˆ†ï¼Œæä¾›åä¾‹
3. è¯†åˆ«è¾¹ç•Œæƒ…å†µå’Œå¯èƒ½è¢«å¿½è§†çš„é£é™©
4. åŸºäºå…·ä½“æ•°æ®æå‡ºæŒ‘æˆ˜ï¼Œè€Œéä¸ºäº†æ‰¹è¯„è€Œæ‰¹è¯„

"""
    else:  # blue team
        prefix = """
ã€è“é˜Ÿè§’è‰² - å»ºè®¾æ€§åˆ†æã€‘
ä½ çš„ä»»åŠ¡æ˜¯æä¾›å®¢è§‚ä¸“ä¸šçš„è¯„ä¼°ï¼š
1. åŸºäºé¢†åŸŸä¸“ä¸šçŸ¥è¯†è¿›è¡Œæ·±åº¦åˆ†æ
2. å¯¹çº¢é˜Ÿçš„åˆç†æ‰¹è¯„äºˆä»¥æ‰¿è®¤å’Œè§£é‡Š
3. å¯¹ä¸åˆç†æ‰¹è¯„æä¾›æœ‰åŠ›çš„æ•°æ®åé©³
4. ä¿æŒå®¢è§‚ï¼Œé¿å…ç›²ç›®é˜²å¾¡

"""
    return prefix + base_prompt

# ========== åˆ›æ–°æœºåˆ¶3: åŠ¨æ€è§’è‰²è½®æ¢ ==========
def rotate_agent_roles(agent_names, round_idx, mode="round_robin", context=None):
    """
    åŠ¨æ€è§’è‰²è½®æ¢
    mode: "round_robin" - è½®è¯¢è½®æ¢
          "adaptive" - åŸºäºä¸Šä¸‹æ–‡è‡ªé€‚åº”
    """
    if mode == "round_robin":
        # ç®€å•è½®æ¢ï¼šæ¯è½®æ‰€æœ‰agentå‘å³ç§»ä¸€ä½
        n = len(agent_names)
        rotated = {}
        for i, name in enumerate(agent_names):
            new_idx = (i + round_idx) % n
            rotated[name] = agent_names[new_idx]
        return rotated
    
    elif mode == "adaptive" and context:
        # åŸºäºä¸Šä¸‹æ–‡è‡ªé€‚åº”åˆ†é…è§’è‰²
        # å¦‚æœä¸Šä¸€è½®å‘ç°é€šä¿¡é—®é¢˜ï¼Œåˆ™åˆ†é…æ›´å¤šé€šä¿¡ä¸“å®¶
        rotated = {}
        for name in agent_names:
            rotated[name] = name  # é»˜è®¤ä¿æŒ
        
        # è¿™é‡Œå¯ä»¥æ ¹æ®contextåŠ¨æ€è°ƒæ•´ï¼Œç®€åŒ–å¤„ç†
        return rotated
    
    return {name: name for name in agent_names}

# ========== åˆ›æ–°æœºåˆ¶4: è¯æ®é“¾è¿½è¸ª ==========
def validate_evidence_chain(structured_response):
    """
    éªŒè¯è¯æ®é“¾çš„å®Œæ•´æ€§
    è¿”å›: (is_valid, missing_components, quality_score)
    """
    checks = {
        "has_claim": bool(structured_response.get("claim", "").strip()),
        "has_evidence": bool(structured_response.get("evidence", "").strip()),
        "has_reasoning": bool(structured_response.get("summary", "").strip()),
        "has_confidence": structured_response.get("confidence") is not None
    }
    
    missing = [k for k, v in checks.items() if not v]
    is_valid = len(missing) == 0
    
    # é¢å¤–æ£€æŸ¥ï¼šè¯æ®ä¸­æ˜¯å¦åŒ…å«æ•°æ®å¼•ç”¨
    evidence_text = structured_response.get("evidence", "")
    has_data_ref = bool(re.search(r'\d+\.?\d*[%m/såº¦kmæ¬¡]', evidence_text))
    
    quality_score = sum(checks.values()) / len(checks)
    if has_data_ref:
        quality_score = min(1.0, quality_score + 0.1)
    
    return is_valid, missing, quality_score

def extract_evidence_references(evidence_text):
    """æå–è¯æ®ä¸­çš„æ•°æ®å¼•ç”¨"""
    # åŒ¹é…æ•°å­—+å•ä½çš„æ¨¡å¼
    pattern = r'(\d+\.?\d*)\s*([%m/såº¦kmæ¬¡msKB]|m/s|km/h)'
    matches = re.findall(pattern, evidence_text)
    return [(float(m[0]), m[1]) for m in matches]

# ========== åˆ›æ–°æœºåˆ¶5: å…±è¯†å»ºæ¨¡ä¸åˆ†æ­§é‡åŒ– ==========
def compute_multi_dimensional_consensus(agents_structured, agent_names):
    """
    è®¡ç®—å¤šç»´åº¦å…±è¯†
    è¿”å›: {dimension: score, overall: score}
    """
    n_agents = len(agents_structured)
    if n_agents < 2:
        return {"overall": 1.0}
    
    consensus = {}
    
    # ç»´åº¦1: ç½®ä¿¡åº¦å…±è¯†ï¼ˆåˆ†æ•°ä¸€è‡´æ€§ï¼‰
    confidences = [s.get("confidence", 0.5) for s in agents_structured]
    conf_std = np.std(confidences)
    consensus["score"] = max(0, 1.0 - conf_std)
    
    # ç»´åº¦2: è¯­ä¹‰ç›¸ä¼¼åº¦
    summaries = [s.get("summary", "") for s in agents_structured]
    similarities = []
    for i in range(n_agents):
        for j in range(i+1, n_agents):
            sim = SequenceMatcher(None, summaries[i], summaries[j]).ratio()
            similarities.append(sim)
    consensus["semantic"] = np.mean(similarities) if similarities else 0.5
    
    # ç»´åº¦3: ä¼˜å…ˆçº§å¯¹é½ï¼ˆåŸºäºclaimç›¸ä¼¼åº¦ï¼‰
    claims = [s.get("claim", "") for s in agents_structured]
    claim_sims = []
    for i in range(n_agents):
        for j in range(i+1, n_agents):
            sim = SequenceMatcher(None, claims[i], claims[j]).ratio()
            claim_sims.append(sim)
    consensus["priority"] = np.mean(claim_sims) if claim_sims else 0.5
    
    # ç»´åº¦4: å…³æ³¨ç‚¹é‡å 
    # ç®€åŒ–ï¼šåŸºäºè¯æ®æ–‡æœ¬çš„é‡å 
    evidences = [s.get("evidence", "") for s in agents_structured]
    ev_words = [set(ev.split()) for ev in evidences if ev]
    if len(ev_words) >= 2:
        intersection = set.intersection(*ev_words)
        union = set.union(*ev_words)
        consensus["concern"] = len(intersection) / max(1, len(union))
    else:
        consensus["concern"] = 0.5
    
    # æ•´ä½“å…±è¯†ï¼ˆåŠ æƒå¹³å‡ï¼‰
    weights = [0.25, 0.3, 0.25, 0.2]
    dims = ["score", "semantic", "priority", "concern"]
    consensus["overall"] = sum(consensus.get(d, 0.5) * w for d, w in zip(dims, weights))
    
    return consensus

def identify_disagreement_types(agents_structured):
    """
    è¯†åˆ«åˆ†æ­§ç±»å‹ï¼šäº‹å®ã€è§£é‡Šã€ä»·å€¼
    """
    disagreements = {"factual": [], "interpretative": [], "value_based": []}
    
    n_agents = len(agents_structured)
    for i in range(n_agents):
        for j in range(i+1, n_agents):
            claim_i = agents_structured[i].get("claim", "")
            claim_j = agents_structured[j].get("claim", "")
            
            # ç®€åŒ–åˆ†ç±»é€»è¾‘
            if claim_i and claim_j:
                sim = SequenceMatcher(None, claim_i, claim_j).ratio()
                if sim < 0.3:
                    # åˆ¤æ–­æ˜¯å¦åŒ…å«æ•°å€¼ï¼ˆäº‹å®åˆ†æ­§ï¼‰
                    has_num_i = bool(re.search(r'\d+', claim_i))
                    has_num_j = bool(re.search(r'\d+', claim_j))
                    if has_num_i and has_num_j:
                        disagreements["factual"].append((i, j, claim_i, claim_j))
                    elif "åº”è¯¥" in claim_i or "åº”è¯¥" in claim_j:
                        disagreements["value_based"].append((i, j, claim_i, claim_j))
                    else:
                        disagreements["interpretative"].append((i, j, claim_i, claim_j))
    
    return disagreements

# ========== åˆ›æ–°æœºåˆ¶6: å…ƒè®¤çŸ¥è´¨é‡ç›‘æ§ ==========
def assess_debate_quality(agents_structured, round_idx):
    """
    è¯„ä¼°è¾©è®ºè´¨é‡
    è¿”å›: {evidence_score, coverage_score, coherence_score, novelty_score, overall}
    """
    quality = {}
    n_agents = len(agents_structured)
    
    # æŒ‡æ ‡1: è¯æ®æä¾›ç‡
    evidence_count = sum(1 for s in agents_structured if s.get("evidence", "").strip())
    quality["evidence"] = evidence_count / max(1, n_agents)
    
    # æŒ‡æ ‡2: è§†è§’è¦†ç›–åº¦ï¼ˆæ£€æŸ¥æ˜¯å¦æ¶µç›–å¤šä¸ªé¢†åŸŸï¼‰
    keywords = {
        "è½¨è¿¹": ["è½¨è¿¹", "è·¯å¾„", "èˆªçº¿"],
        "ååŒ": ["ååŒ", "ç¼–é˜Ÿ", "é€šä¿¡"],
        "å®‰å…¨": ["å®‰å…¨", "é£é™©", "é¿ç¢°"],
        "æ•ˆç‡": ["æ•ˆç‡", "èƒ½è€—", "æ—¶é—´"]
    }
    covered_aspects = set()
    for s in agents_structured:
        text = s.get("summary", "") + s.get("evidence", "")
        for aspect, kws in keywords.items():
            if any(kw in text for kw in kws):
                covered_aspects.add(aspect)
    quality["coverage"] = len(covered_aspects) / len(keywords)
    
    # æŒ‡æ ‡3: é€»è¾‘è¿è´¯æ€§ï¼ˆæ£€æµ‹å¾ªç¯è®ºè¯ï¼‰
    # ç®€åŒ–ï¼šæ£€æŸ¥claimå’Œevidenceæ˜¯å¦æœ‰æ˜æ˜¾é‡å¤
    coherence_scores = []
    for s in agents_structured:
        claim = s.get("claim", "")
        evidence = s.get("evidence", "")
        if claim and evidence:
            # å¦‚æœclaimå’Œevidenceå®Œå…¨ç›¸åŒï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯
            sim = SequenceMatcher(None, claim, evidence).ratio()
            coherence_scores.append(1.0 - min(0.5, sim))
    quality["coherence"] = np.mean(coherence_scores) if coherence_scores else 0.5
    
    # æŒ‡æ ‡4: æ–°é¢–åº¦ï¼ˆä¸ä¸Šä¸€è½®æ¯”è¾ƒï¼‰
    if round_idx == 0:
        quality["novelty"] = 1.0  # ç¬¬ä¸€è½®é»˜è®¤æ–°é¢–
    else:
        quality["novelty"] = 0.8  # åç»­è½®æ¬¡éœ€è¦å®é™…æ¯”è¾ƒï¼Œè¿™é‡Œç®€åŒ–
    
    # æ•´ä½“è´¨é‡åˆ†æ•°
    quality["overall"] = min(
        quality["evidence"],
        quality["coverage"],
        quality["coherence"],
        quality["novelty"]
    )
    
    return quality

def generate_quality_intervention(quality_metrics):
    """
    æ ¹æ®è´¨é‡æŒ‡æ ‡ç”Ÿæˆå¹²é¢„å»ºè®®
    """
    interventions = []
    threshold = DEBATE_CONFIG["meta_cognitive"]["quality_threshold"]
    
    if quality_metrics["evidence"] < threshold:
        interventions.append("âš ï¸ è¯æ®ä¸è¶³ï¼šè¯·å„ä½ä¸“å®¶æä¾›æ›´å¤šå…·ä½“æ•°æ®æ”¯æŒ")
    
    if quality_metrics["coverage"] < threshold:
        interventions.append("âš ï¸ è§†è§’ä¸å…¨ï¼šè¯·å…³æ³¨é—æ¼çš„è¯„ä¼°ç»´åº¦")
    
    if quality_metrics["coherence"] < threshold:
        interventions.append("âš ï¸ é€»è¾‘é—®é¢˜ï¼šè¯·é¿å…å¾ªç¯è®ºè¯ï¼Œç¡®ä¿è¯æ®ç‹¬ç«‹äºç»“è®º")
    
    if quality_metrics["novelty"] < 0.3:
        interventions.append("âš ï¸ é‡å¤è®¨è®ºï¼šå»ºè®®æ—©åœæˆ–è½¬æ¢è®¨è®ºè§’åº¦")
    
    return interventions


def call_glm_api(prompt, api_key, agent_role=""):
    base_url = os.environ.get("ZHIPU_BASE_URL", "").strip()
    client_kwargs = {"api_key": api_key}
    if base_url:
        client_kwargs["base_url"] = base_url
    client = ZhipuAiClient(**client_kwargs)
    messages = [
        {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚{agent_role}".strip()},
        {"role": "user", "content": prompt}
    ]
    response = client.chat.completions.create(
        model="glm-4.6",
        messages=messages,
        temperature=0.6
    )
    return response.choices[0].message.content


def construct_message(agents_responses, question, agent_id):
    """
    ç”Ÿæˆç»“æ„åŒ–è·Ÿè¿›æç¤ºï¼Œè¦æ±‚è¾“å‡ºå›ºå®šå—ä»¥æå‡ä¿¡æ¯å¯†åº¦ï¼š
    [CLAIM][EVIDENCE][COUNTER][SUMMARY][CONFIDENCE]
    å¹¶æŒ‡å®šä¸€ä¸ªè´¨è¯¢ç›®æ ‡ä»¥ä¿ƒä½¿ç›´æ¥åé©³ã€‚
    """
    if not agents_responses:
        return (
            f"é—®é¢˜: {question}\n\n"
            "è¯·ä½¿ç”¨ä»¥ä¸‹ä¸¥æ ¼ç»“æ„åŒ–æ ¼å¼(ASCII):\n"
            "[CLAIM] ä¸€å¥æ ¸å¿ƒåˆ¤æ–­\n"
            "[EVIDENCE] 3-5æ¡è¯æ®(å«å…·ä½“æ•°å€¼)\n"
            "[COUNTER] é’ˆå¯¹æ½œåœ¨åé©³çš„æœ€å°å˜æ›´å›åº”\n"
            "[SUMMARY] 3ç‚¹ç»¼åˆ + 1æ¡è¡ŒåŠ¨å»ºè®®\n"
            "[CONFIDENCE] 0.00~1.00\n"
        )

    target_id = (agent_id + 1) % max(1, len(agents_responses))
    prefix = [
        f"é—®é¢˜: {question}",
        "ä¸Šä¸€è½®å…¶ä»–ä¸“å®¶çš„å›ç­”è¦ç‚¹(æˆªæ–­):",
    ]
    for i, response in enumerate(agents_responses):
        if i != agent_id:
            preview = str(response).strip().replace("\n", " ")
            prefix.append(f"- ä¸“å®¶{i+1}: {preview[:400]}")
    prefix.append(
        "\nè¯·ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹ç»“æ„åŒ–æ ¼å¼(ASCII):\n"
        "[CLAIM] ä¸€å¥æ ¸å¿ƒåˆ¤æ–­\n"
        "[EVIDENCE] 3-5æ¡è¯æ®(å«å…·ä½“æ•°å€¼)\n"
        f"[COUNTER] é’ˆå¯¹ä¸“å®¶{target_id+1}å…³é”®è®ºç‚¹çš„åé©³æˆ–è¾¹ç•Œæ¡ä»¶\n"
        "[SUMMARY] 3ç‚¹ç»¼åˆ + 1æ¡è¡ŒåŠ¨å»ºè®®\n"
        "[CONFIDENCE] 0.00~1.00\n"
    )
    return "\n".join(prefix)


def load_trajectory_from_pkl(file_path):
    """
    ä» trajectory.pkl åŠ è½½è½¨è¿¹å¹¶è½¬æ¢ä¸º flight_data ç»“æ„
    æ”¯æŒ numpy.ndarrayã€dictã€list ç­‰å¸¸è§æ ¼å¼
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è½¨è¿¹æ–‡ä»¶: {file_path}")
    with open(file_path, 'rb') as f:
        data = pickle.load(f)

    # æå–æ•°ç»„ï¼ˆæ”¯æŒåµŒå¥—ç»“æ„ï¼‰
    preferred_keys = {"trajectory", "positions", "observations", "coords", "path"}
    found = []

    def is_numeric(x):
        return isinstance(x, (int, float, np.integer, np.floating))

    def is_numeric_list(lst):
        return isinstance(lst, list) and len(lst) > 0 and all(is_numeric(e) for e in lst)

    def is_numeric_2d_list(lst):
        return isinstance(lst, (list, tuple)) and len(lst) > 0 and all(
            isinstance(e, (list, tuple)) and len(e) > 0 and all(is_numeric(v) for v in e)
            for e in lst
        )

    def collect(obj, path=""):
        if isinstance(obj, np.ndarray):
            found.append((obj, path))
        elif isinstance(obj, dict):
            for k, v in obj.items():
                new_path = f"{path}.{k}" if path else str(k)
                collect(v, new_path)
        elif isinstance(obj, (list, tuple)):
            if is_numeric_2d_list(obj) or is_numeric_list(obj):
                found.append((np.array(obj, dtype=np.float64), path))
            else:
                for i, v in enumerate(obj):
                    new_path = f"{path}[{i}]" if path else f"[{i}]"
                    collect(v, new_path)

    collect(data)

    arr = None
    if found:
        # ä¼˜å…ˆé€‰æ‹©è·¯å¾„ä¸­åŒ…å«å¸¸è§é”®åçš„æ•°ç»„ï¼Œå…¶æ¬¡é€‰æ‹©æœ€å¤§æ•°ç»„
        prioritized = [a for a in found if any(k in a[1] for k in preferred_keys)]
        if prioritized:
            arr = max(prioritized, key=lambda x: x[0].size)[0]
        else:
            arr = max(found, key=lambda x: x[0].size)[0]

    # è½¨è¿¹å½¢çŠ¶å¤„ç†ï¼šè‡³å°‘éœ€è¦ Xã€Yï¼ŒZ å¯é€‰
    if arr.ndim == 1:
        arr = arr.reshape(-1, 2)
    elif arr.shape[1] < 2:
        # å¦‚æœåˆ—ä¸è¶³ï¼Œé‡å¤æˆ–å¡«å……åˆ°2åˆ—
        arr = np.pad(arr, ((0,0),(0, max(0,2-arr.shape[1]))), mode='edge')

    # ç”Ÿæˆå•æ— äººæœºçš„è½¨è¿¹ç‚¹
    traj = []
    total_distance = 0.0
    prev_xy = None
    max_speed = 0.0

    # æ—¶é—´æ­¥å‡è®¾ä¸º1ç§’
    for i in range(arr.shape[0]):
        x = float(arr[i, 0])
        y = float(arr[i, 1])
        alt = float(arr[i, 2]) if arr.shape[1] >= 3 else 120.0 + 5.0 * np.sin(i / 20.0)

        if prev_xy is None:
            speed = 0.0
            heading_deg = 0.0
        else:
            dx = x - prev_xy[0]
            dy = y - prev_xy[1]
            step_dist = float(np.hypot(dx, dy))
            total_distance += step_dist
            speed = step_dist  # å•ä½æ—¶é—´æ­¥è·ç¦»ï¼Œè§†ä¸ºé€Ÿåº¦
            heading_rad = np.arctan2(dy, dx)
            heading_deg = (np.degrees(heading_rad) + 360.0) % 360.0
            max_speed = max(max_speed, speed)
        prev_xy = (x, y)

        traj.append({
            "gps": {"lat": x, "lon": y},
            "altitude": alt,
            "speed": speed,
            "heading": heading_deg,
            "timestamp": i
        })

    avg_speed = float(np.mean([p["speed"] for p in traj]))
    battery_consumption = round(total_distance * 0.02 + len(traj) * 0.005, 2)  # ç®€å•ä¼°ç®—

    flight_data = {
        "mission_id": f"TRJ-{time.strftime('%Y%m%d%H%M%S')}",
        "mission_type": "è½¨è¿¹åˆ†æ",
        "drone_count": 1,
        "flight_duration": f"{len(traj)} æ­¥",
        "mission_info": {"duration_minutes": max(1, len(traj)//2)},
        "drones": [
            {
                "id": "DRONE-001",
                "trajectory": traj,
                "performance_metrics": {
                    "avg_speed": round(avg_speed, 3),
                    "max_speed": round(max_speed, 3),
                    "battery_consumption": battery_consumption,
                    "distance_traveled": round(total_distance, 3)
                },
                "anomalies": []
            }
        ],
        "swarm_metrics": {
            "formation_stability": 100.0,  # å•æœºè¿‘ä¼¼æ»¡åˆ†
            "communication_success_rate": 100.0,
            "coordination_delay_avg": 10.0,
            "task_completion_rate": 100.0,
            "collision_avoidance_events": 0
        }
    }

    return flight_data

# æå–è½¨è¿¹ç‚¹çš„é€šç”¨å­—æ®µ
def _extract_xy_alt_speed(traj_point):
    # æ”¯æŒä¸¤ç§ç»“æ„ï¼š{gps:{lat,lon}} æˆ– {latitude, longitude}
    if "gps" in traj_point:
        x = float(traj_point["gps"].get("lat", 0.0))
        y = float(traj_point["gps"].get("lon", 0.0))
    else:
        x = float(traj_point.get("latitude", traj_point.get("lat", 0.0)))
        y = float(traj_point.get("longitude", traj_point.get("lon", 0.0)))
    alt = float(traj_point.get("altitude", 0.0))
    speed = float(traj_point.get("speed", 0.0))
    heading = float(traj_point.get("heading", 0.0))
    t = int(traj_point.get("timestamp", 0))
    return x, y, alt, speed, heading, t

# èˆªå‘è½¬æŒ‡å—é’ˆæ–¹å‘
def _compass_from_heading(h):
    h = (h % 360.0)
    best = "N"; best_diff = 1e9
    for name, deg in COMPASS_DIRS:
        diff = min(abs(h - deg), 360 - abs(h - deg))
        if diff < best_diff:
            best_diff = diff
            best = name
    return best

# ç®€æ˜“ Ramerâ€“Douglasâ€“Peucker ç®—æ³•
def _rdp(points, epsilon=0.001):
    # ç®€æ˜“ Ramerâ€“Douglasâ€“Peuckerï¼Œå¤šæ•°åœºæ™¯å¤Ÿç”¨
    if len(points) < 3:
        return points
    def _perp_dist(pt, a, b):
        ax, ay = a; bx, by = b; px, py = pt
        dx, dy = bx - ax, by - ay
        if dx == 0 and dy == 0:
            return math.hypot(px - ax, py - ay)
        t = ((px - ax) * dx + (py - ay) * dy) / (dx * dx + dy * dy)
        t = max(0, min(1, t))
        cx, cy = ax + t * dx, ay + t * dy
        return math.hypot(px - cx, py - cy)
    def _rdp_rec(pts):
        if len(pts) <= 2:
            return pts
        a, b = pts[0], pts[-1]
        idx, dmax = 0, -1
        for i in range(1, len(pts) - 1):
            d = _perp_dist(pts[i], a, b)
            if d > dmax:
                idx, dmax = i, d
        if dmax > epsilon:
            left = _rdp_rec(pts[:idx+1])
            right = _rdp_rec(pts[idx:])
            return left[:-1] + right
        else:
            return [a, b]
    return _rdp_rec(points)

# ä¸º LLM ç”Ÿæˆè½¨è¿¹æ‘˜è¦
def summarize_trajectory_for_llm(flight_data, segments=6, max_events=10, rdp_epsilon=0.001, max_waypoints=8):
    traj = flight_data.get("drones", [{}])[0].get("trajectory", [])
    if not traj:
        return {"meta": {}, "stats": {}, "segments": [], "events": [], "waypoints": []}
    xs, ys, alts, speeds, headings, times = [], [], [], [], [], []
    for p in traj:
        x, y, alt, spd, hdg, t = _extract_xy_alt_speed(p)
        xs.append(x); ys.append(y); alts.append(alt); speeds.append(spd); headings.append(hdg); times.append(t)
    n = len(traj)
    duration_s = (times[-1] - times[0]) if times else n
    # è·ç¦»ï¼ˆè¿‘ä¼¼æ¬§æ°ï¼‰
    dist = 0.0
    turns = 0
    accel_events = 0
    decel_events = 0
    sharp_turns = []
    climb_events = []
    for i in range(1, n):
        dist += math.hypot(xs[i] - xs[i-1], ys[i] - ys[i-1])
        dh = abs(headings[i] - headings[i-1])
        dh = min(dh, 360 - dh)
        if dh >= 30:
            turns += 1
            sharp_turns.append({"t": times[i], "angle_deg": round(dh, 1)})
        dv = speeds[i] - speeds[i-1]
        if dv >= 2.0:
            accel_events += 1
        elif dv <= -2.0:
            decel_events += 1
        # çˆ¬å‡äº‹ä»¶ï¼šè¿ç»­ä¸Šå‡é€Ÿç‡é˜ˆå€¼
        rate = (alts[i] - alts[i-1]) / max(1, (times[i] - times[i-1]))
        if rate >= 3.0:
            climb_events.append({"t": times[i], "rate_mps": round(rate, 2)})
    speed_mean = float(np.mean(speeds)) if speeds else 0.0
    speed_std = float(np.std(speeds)) if speeds else 0.0
    speed_max = float(np.max(speeds)) if speeds else 0.0
    alt_mean = float(np.mean(alts)) if alts else 0.0
    alt_std = float(np.std(alts)) if alts else 0.0
    heading_std = float(np.std(headings)) if headings else 0.0
    smoothness_index = 1.0 / (1.0 + heading_std / 90.0)
    # åˆ†æ®µ
    seg_len = max(1, n // segments)
    segs = []
    for s in range(segments):
        i0 = s * seg_len
        i1 = min(n, (s+1) * seg_len)
        if i0 >= n:
            break
        v_mean = float(np.mean(speeds[i0:i1])) if i1 > i0 else 0.0
        v_std = float(np.std(speeds[i0:i1])) if i1 > i0 else 0.0
        a_mean = float(np.mean(alts[i0:i1])) if i1 > i0 else 0.0
        a_std = float(np.std(alts[i0:i1])) if i1 > i0 else 0.0
        h_mean = float(np.mean(headings[i0:i1])) if i1 > i0 else 0.0
        dir_str = _compass_from_heading(h_mean)
        turns_seg = 0
        for j in range(i0+1, i1):
            dh = abs(headings[j] - headings[j-1]); dh = min(dh, 360 - dh)
            if dh >= 30: turns_seg += 1
        turn_intensity = "low" if turns_seg <= 1 else ("medium" if turns_seg <= 3 else "high")
        segs.append({
            "t0": int(times[i0]), "t1": int(times[i1-1]) if i1 > i0 else int(times[i0]),
            "dir": dir_str,
            "v_mean": round(v_mean, 2), "v_std": round(v_std, 2),
            "alt_mean": round(a_mean, 1), "alt_std": round(a_std, 1),
            "turn_intensity": turn_intensity
        })
    # äº‹ä»¶æ—¶é—´è½´ï¼ˆè£å‰ªï¼‰
    events = []
    for e in sharp_turns[:max_events]:
        events.append({"t": e["t"], "type": "sharp_turn", "angle_deg": e["angle_deg"]})
    for e in climb_events[:max_events - len(events)]:
        events.append({"t": e["t"], "type": "climb", "rate_mps": e["rate_mps"]})
    # Waypoints via RDP
    pts = list(zip(xs, ys))
    wps = _rdp(pts, epsilon=rdp_epsilon)
    if len(wps) > max_waypoints:
        step = max(1, len(wps) // max_waypoints)
        wps = wps[::step][:max_waypoints]
    summary = {
        "meta": {"duration_s": int(duration_s), "n_points": n},
        "stats": {
            "distance_km": round(dist / 1000.0, 3),
            "speed_mean_mps": round(speed_mean, 3), "speed_std_mps": round(speed_std, 3), "speed_max_mps": round(speed_max, 3),
            "alt_mean_m": round(alt_mean, 1), "alt_std_m": round(alt_std, 1),
            "turn_count_gt30deg": int(turns),
            "smoothness_index": round(smoothness_index, 3)
        },
        "segments": segs,
        "events": events,
        "waypoints": [[round(a,6), round(b,6)] for a,b in wps]
    }
    return summary

# å°†æ‘˜è¦æ ¼å¼åŒ–ä¸ºç´§å‡‘ DSL
def format_llm_dsl(summary, scores=None):
    meta = summary.get("meta", {})
    stats = summary.get("stats", {})
    segs = summary.get("segments", [])
    events = summary.get("events", [])
    wps = summary.get("waypoints", [])
    lines = []
    lines.append(f"META: dur={meta.get('duration_s',0)}s, pts={meta.get('n_points',0)}")
    lines.append(
        "STATS: "
        f"L={stats.get('distance_km',0)}km, "
        f"v={stats.get('speed_mean_mps',0)}Â±{stats.get('speed_std_mps',0)}m/s, "
        f"vmax={stats.get('speed_max_mps',0)}m/s, "
        f"alt={stats.get('alt_mean_m',0)}Â±{stats.get('alt_std_m',0)}m, "
        f"turns>30Â°={stats.get('turn_count_gt30deg',0)}, "
        f"smooth={stats.get('smoothness_index',0)}"
    )
    for i, s in enumerate(segs[:8], 1):
        lines.append(
            f"SEG[{i}]: t={s.get('t0',0)}-{s.get('t1',0)}, dir={s.get('dir','')}, "
            f"v={s.get('v_mean',0)}Â±{s.get('v_std',0)}, alt={s.get('alt_mean',0)}Â±{s.get('alt_std',0)}, "
            f"turn={s.get('turn_intensity','low')}"
        )
    for e in events[:10]:
        if e.get('type') == 'sharp_turn':
            lines.append(f"EVENT: t={e['t']}, sharp_turn={e['angle_deg']}deg")
        elif e.get('type') == 'climb':
            lines.append(f"EVENT: t={e['t']}, climb={e['rate_mps']}m/s")
    if wps:
        wp_str = "â†’".join([f"({a},{b})" for a,b in wps])
        lines.append(f"WAYPTS: {wp_str}")
    if scores:
        try:
            sc = scores.get('swarm_coordination', {})
            if sc.get('not_applicable'):
                lines.append("SCORES: COORD=N/A (å•æœºä»»åŠ¡)")
            else:
                lines.append(
                    "SCORES: "
                    f"formation_stab={sc.get('formation_stability',0)}, "
                    f"comm_quality={sc.get('communication_quality',0)}, "
                    f"coord_delay={sc.get('coordination_delay',0)}, "
                    f"task_comp={sc.get('task_completion',0)}"
                )
        except Exception:
            pass
    return "\n".join(lines)


def main():
    from debate_protocol import structured_prompt_first_round, construct_structured_followup, parse_structured_response, update_agent_weights, summary_similarity
    api_key = 'd2811fc4f03f48f2bb547d6a6b3378f4.GtaNMZOyqulNGa1L'
    print("ğŸš æ­£åœ¨åŠ è½½è½¨è¿¹æ–‡ä»¶: c:\\Users\\bafs\\Desktop\\llm_debate\\trajectory.pkl")
    flight_data = load_trajectory_from_pkl(r"c:\\Users\\bafs\\Desktop\\llm_debate\\trajectory.pkl")
    print("âœ… å·²ä» trajectory.pkl åŠ è½½è½¨è¿¹æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸ºè¯„ä¼°ç»“æ„")


    # ä¿å­˜é£è¡Œæ•°æ®
    with open("drone_flight_data.json", "w", encoding="utf-8") as f:
        json.dump(flight_data, f, ensure_ascii=False, indent=2)
    print("âœ… é£è¡Œæ•°æ®å·²ä¿å­˜åˆ° drone_flight_data.json")
    
    # è®¾ç½®è¾©è®ºå‚æ•°
    agents = 0  # å ä½ï¼Œç¨åæ ¹æ®AGENTSé‡è®¾
    rounds = 2  # 2è½®è¾©è®º
    
    # æ™ºèƒ½ä½“è§’è‰²è®¾å®š - ç§»åŠ¨åˆ°mainå‡½æ•°å¼€å§‹å¤„
    global AGENTS
    AGENTS = {

        "ç¾¤èšèƒ½åŠ›ä¸“å®¶": {
            "role": "ç¾¤èšèƒ½åŠ›ä¸“å®¶",
            "expertise": "ç¾¤ä½“è¡Œä¸ºå»ºæ¨¡ã€é˜Ÿå½¢ä¿æŒã€å¯†åº¦æ§åˆ¶ä¸é‚»åŸŸäº¤äº’",
            "scoring_focus": ["formation_stability", "communication_quality", "coordination_delay", "task_completion"],
            "evaluation_prompt": "ä½œä¸ºç¾¤èšèƒ½åŠ›ä¸“å®¶ï¼Œè¯·è¯„ä¼°ç¾¤ä½“èšåˆã€é˜Ÿå½¢ä¿æŒä¸ååŒè¡Œä¸ºè´¨é‡ï¼Œç»“åˆç¼–é˜Ÿç¨³å®šæ€§ã€é€šä¿¡è´¨é‡ã€åè°ƒå»¶è¿Ÿä¸ä»»åŠ¡å®Œæˆåº¦å››é¡¹æŒ‡æ ‡ï¼Œå¼•ç”¨å…·ä½“è¯æ®å¹¶ç»™å‡ºä½æˆæœ¬çš„ç¾¤èšç­–ç•¥æ”¹è¿›å»ºè®®ï¼›å•æœºä»»åŠ¡ä¸‹ååŒç›¸å…³é¡¹è§†ä¸ºN/Aã€‚"
        },
         "é›†ç¾¤ååŒä¸é€šä¿¡å·¥ç¨‹å¸ˆ": {
             "role": "é›†ç¾¤ååŒä¸é€šä¿¡å·¥ç¨‹å¸ˆ",
             "expertise": "é˜Ÿå½¢æ§åˆ¶ã€ä»»åŠ¡åˆ†é…ã€ç½‘ç»œé€šä¿¡ä¸é“¾è·¯è´¨é‡",
             "scoring_focus": ["formation_stability", "communication_quality", "coordination_delay", "task_completion"],
             "evaluation_prompt": "ä½œä¸ºé›†ç¾¤ååŒä¸é€šä¿¡å·¥ç¨‹å¸ˆï¼Œè¯·é‡ç‚¹è¯„ä¼°ç¼–é˜Ÿç¨³å®šæ€§ã€é€šä¿¡è´¨é‡ã€åè°ƒå»¶è¿Ÿå’Œä»»åŠ¡å®Œæˆåº¦ï¼›è‹¥ä¸ºå•æœºä»»åŠ¡è¯·æ˜ç¡®ååŒé¡¹ä¸ºN/Aï¼›å¼•ç”¨å…·ä½“æŒ‡æ ‡å¹¶æå‡ºè½»é‡çº§ååŒç­–ç•¥è°ƒæ•´ã€‚"
         },
         "ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ä¸“å®¶": {
             "role": "ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ä¸“å®¶",
             "expertise": "ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œã€é‡Œç¨‹ç¢‘ç®¡ç†ã€èµ„æºä¸è½½è·è°ƒåº¦",
             "scoring_focus": ["task_completion", "coordination_delay", "communication_quality"],
             "evaluation_prompt": "ä½œä¸ºä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ä¸“å®¶ï¼Œè¯·ç»“åˆä»»åŠ¡å®Œæˆç‡ã€åè°ƒå»¶è¿Ÿä¸é€šä¿¡è´¨é‡ï¼Œè¯†åˆ«å½±å“ä»»åŠ¡è¾¾æˆçš„ç“¶é¢ˆï¼Œå¹¶æå‡ºå¯æ‰§è¡Œçš„ä¼˜åŒ–å»ºè®®ï¼›å•æœºä»»åŠ¡ä¸‹ååŒç›¸å…³é¡¹è§†ä¸ºN/Aã€‚"
         }
     }
    # æ ¹æ®æ–°çš„ä¸“å®¶é›†åˆé‡è®¾æ•°é‡
    agents = len(AGENTS)
     
     # ä¿æŒå‘åå…¼å®¹çš„è§’è‰²æè¿°
    agent_roles = [
        "ä½œä¸ºç¾¤èšèƒ½åŠ›ä¸“å®¶ï¼Œä»ç¾¤ä½“èšåˆä¸ååŒè§’åº¦",
        "ä½œä¸ºé›†ç¾¤ååŒä¸é€šä¿¡å·¥ç¨‹å¸ˆï¼Œä»åä½œä¸ç½‘ç»œè§’åº¦", 
        "ä½œä¸ºä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ä¸“å®¶ï¼Œä»ä»»åŠ¡æ‰§è¡Œä¸è¾¾æˆè§’åº¦"
    ]
    
    # ========== åˆ›æ–°æœºåˆ¶å¯åŠ¨ ==========
    print(f"\nğŸš€ å¯ç”¨åˆ›æ–°è¾©è®ºæœºåˆ¶:")
    print(f"  âœ… åˆ†å±‚è¾©è®ºç»“æ„: {DEBATE_CONFIG['hierarchical']['enabled']}")
    print(f"  âœ… å¯¹æŠ—-åä½œåè®®: {DEBATE_CONFIG['adversarial']['enabled']}")
    print(f"  âœ… åŠ¨æ€è§’è‰²è½®æ¢: {DEBATE_CONFIG['role_rotation']['enabled']}")
    print(f"  âœ… è¯æ®é“¾è¿½è¸ª: {DEBATE_CONFIG['evidence_chain']['enabled']}")
    print(f"  âœ… å…±è¯†å»ºæ¨¡: {DEBATE_CONFIG['consensus_modeling']['enabled']}")
    print(f"  âœ… å…ƒè®¤çŸ¥ç›‘æ§: {DEBATE_CONFIG['meta_cognitive']['enabled']}")
    
    # åˆå§‹åŒ–æ™ºèƒ½ä½“å›ç­”
    agents_responses = [[] for _ in range(agents)]
    agents_structured = [[] for _ in range(agents)]
    agent_weights = [1.0 / agents] * agents
    weights_history = []
    consensus_history = []
    quality_history = []
    team_assignments_history = []
    
    print(f"\nğŸ¤– æ™ºè°±GLM-4.6 æ— äººæœºé›†ç¾¤è¯„ä¼°ç³»ç»Ÿ")
    print(f"è¾©è®ºé—®é¢˜: {DEBATE_QUESTION}\n")
        
    # æ˜¾ç¤ºé£è¡Œæ•°æ®æ‘˜è¦
    print("ğŸ“Š é£è¡Œæ•°æ®æ‘˜è¦:")
    print(f"  - ä»»åŠ¡ID: {flight_data['mission_id']}")
    print(f"  - ä»»åŠ¡ç±»å‹: {flight_data['mission_type']}")
    print(f"  - æ— äººæœºæ•°é‡: {flight_data['drone_count']} æ¶")
    print(f"  - é£è¡Œæ—¶é•¿: {flight_data['flight_duration']}")
    print(f"  - é›†ç¾¤ç¨³å®šæ€§: {'N/A(å•æœº)' if flight_data['drone_count'] <= 1 else str(flight_data['swarm_metrics']['formation_stability']) + '%'}")
    print(f"  - ä»»åŠ¡å®Œæˆç‡: {'N/A(å•æœº)' if flight_data['drone_count'] <= 1 else str(flight_data['swarm_metrics']['task_completion_rate']) + '%'}")
    
    print("\nå‚ä¸è¯„ä¼°çš„ä¸“å®¶:")
    for i, role in enumerate(agent_roles):
        print(f"  ä¸“å®¶ {i+1}: {role}")
    print("\n" + "="*60)
    
    # ç”Ÿæˆé£è¡Œæ•°æ®æ‘˜è¦
    swarm_stab_str = "N/A(å•æœº)" if flight_data['drone_count'] <= 1 else f"{flight_data['swarm_metrics']['formation_stability']}%"
    comm_rate_str = "N/A(å•æœº)" if flight_data['drone_count'] <= 1 else f"{flight_data['swarm_metrics']['communication_success_rate']}%"
    task_comp_str = "N/A(å•æœº)" if flight_data['drone_count'] <= 1 else f"{flight_data['swarm_metrics']['task_completion_rate']}%"
    avoid_events_str = "N/A(å•æœº)" if flight_data['drone_count'] <= 1 else f"{flight_data['swarm_metrics']['collision_avoidance_events']} æ¬¡"
    coord_delay_str = "N/A(å•æœº)" if flight_data['drone_count'] <= 1 else f"{flight_data['swarm_metrics']['coordination_delay_avg']} ms"
    flight_summary = f"""
    åŸºäºä»¥ä¸‹æ— äººæœºé›†ç¾¤é£è¡Œæ•°æ®è¿›è¡Œåˆ†æï¼š
    - ä»»åŠ¡ç±»å‹ï¼š{flight_data['mission_type']}
    - æ— äººæœºæ•°é‡ï¼š{flight_data['drone_count']} æ¶
    - é£è¡Œæ—¶é•¿ï¼š{flight_data['flight_duration']}
    - é›†ç¾¤ç¨³å®šæ€§ï¼š{swarm_stab_str}
    - é€šä¿¡æˆåŠŸç‡ï¼š{comm_rate_str}
    - ä»»åŠ¡å®Œæˆç‡ï¼š{task_comp_str}
    - é¿ç¢°äº‹ä»¶ï¼š{avoid_events_str}
    - å¹³å‡åè°ƒå»¶è¿Ÿï¼š{coord_delay_str}"""
        
        # æ›´æ–°æ™ºèƒ½ä½“å¾ªç¯ä»¥ä½¿ç”¨æ–°çš„AGENTSå­—å…¸
    agent_names = list(AGENTS.keys())
        
        # ç”Ÿæˆè‡ªåŠ¨è¯„åˆ†
    print("æ­£åœ¨è®¡ç®—é£è¡Œæ•°æ®è¯„åˆ†...")
    scores = calculate_flight_scores(flight_data)
    weighted_scores = calculate_weighted_scores(scores)
    
    # ä¸ºLLMç”Ÿæˆç´§å‡‘è½¨è¿¹è¯æ®DSL
    traj_summary = summarize_trajectory_for_llm(flight_data)
    evidence_text = format_llm_dsl(traj_summary, scores=scores)
    try:
        attn = compute_traj_attention(flight_data.get("drones", [{}])[0].get("trajectory", []))
        evidence_text = evidence_text + "\n" + build_attn_dsl_block(flight_data.get("drones", [{}])[0].get("trajectory", []), attn)
        print("ğŸ§ª å·²ç”Ÿæˆè½¨è¿¹æ‘˜è¦DSL(å·²æ³¨å…¥åˆ°æç¤º): META/SEG/EVENT/WAYPTS/SCORES/ATTN")
    except Exception:
        print("âš ï¸ æ³¨æ„åŠ›æ‘˜è¦æ„å»ºå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹DSL")
    # æ‰“å°æŠ½è±¡ç»“æœï¼šJSONä¸DSL
    print("\n====== è½¨è¿¹æ‘˜è¦(JSON) ======")
    print(json.dumps(traj_summary, ensure_ascii=False, indent=2))
    print("\n====== è½¨è¿¹æ‘˜è¦(DSL) ======")
    print(evidence_text)
    print("="*60)
    
    # ç”Ÿæˆä¸“å®¶è¯„åˆ†
    expert_scorings = []
    for agent_name in agent_names:
        expert_scoring = generate_expert_scoring(agent_name, flight_data, scores)
        expert_scorings.append(expert_scoring)
    
    print(f"è‡ªåŠ¨è¯„åˆ†å®Œæˆï¼Œæ€»åˆ†: {weighted_scores['total_score']:.2f}")
    
    # ========== åˆ›æ–°æœºåˆ¶1: è¯„ä¼°é—®é¢˜å¤æ‚åº¦å¹¶è·¯ç”±åˆ°è¾©è®ºå±‚ ==========
    complexity = assess_issue_complexity(flight_data, scores, weighted_scores)
    debate_layer = route_to_debate_layer(complexity)
    max_rounds_for_layer = DEBATE_CONFIG["hierarchical"]["max_rounds"][debate_layer]
    
    print(f"\nğŸ¯ é—®é¢˜å¤æ‚åº¦: {complexity:.2f}")
    print(f"ğŸ“ è·¯ç”±åˆ°: {debate_layer}")
    print(f"ğŸ”¢ æœ€å¤§è¾©è®ºè½®æ•°: {max_rounds_for_layer}")
    
    actual_rounds = min(rounds, max_rounds_for_layer)
    
    # è¿›è¡Œè¾©è®º
    for round_idx in range(actual_rounds):
        print(f"\nğŸ”¥ ç¬¬ {round_idx+1}/{actual_rounds} è½®è¯„ä¼° [{debate_layer}]")
        print("-" * 60)
        
        # ========== åˆ›æ–°æœºåˆ¶2: åˆ†é…çº¢è“é˜Ÿ ==========
        if DEBATE_CONFIG["adversarial"]["enabled"]:
            team_assignments = assign_red_blue_teams(agent_names, round_idx)
            team_assignments_history.append(team_assignments)
            print("ğŸ­ çº¢è“é˜Ÿåˆ†é…:")
            for name, team in team_assignments.items():
                emoji = "ğŸ”´" if team == "red" else "ğŸ”µ"
                print(f"  {emoji} {name}: {'çº¢é˜Ÿ(æ‰¹åˆ¤)' if team == 'red' else 'è“é˜Ÿ(å»ºè®¾)'}")
        else:
            team_assignments = {name: "blue" for name in agent_names}
        
        print()
        
        for agent_idx in range(agents):
            agent_name = agent_names[agent_idx]
            agent_info = AGENTS[agent_name]
            agent_team = team_assignments.get(agent_name, "blue")
            
            print(f"\nğŸ’­ {agent_name} ({'ğŸ”´çº¢é˜Ÿ' if agent_team=='red' else 'ğŸ”µè“é˜Ÿ'}) æ­£åœ¨åˆ†æ...")
            
            # æ„å»ºæç¤ºï¼ŒåŒ…å«é£è¡Œæ•°æ®å’Œè¯„åˆ†
            if round_idx == 0:
                # ç¬¬ä¸€è½®ï¼šåŸºäºé£è¡Œæ•°æ®å’Œè‡ªåŠ¨è¯„åˆ†è¿›è¡Œåˆ†æ
                base_prompt = structured_prompt_first_round(
                    agent_info,
                    flight_summary,
                    weighted_scores,
                    expert_scorings[agent_idx],
                    DEBATE_QUESTION,
                    evidence_text=evidence_text,
                )
            else:
                # åç»­è½®æ¬¡ï¼Œè€ƒè™‘å…¶ä»–ä¸“å®¶çš„åˆ†æ
                last_round_responses = [agents_responses[i][round_idx-1] for i in range(agents)]
                base_prompt = construct_structured_followup(
                    last_round_responses,
                    DEBATE_QUESTION,
                    agent_idx,
                    weighted_scores,
                    expert_scorings[agent_idx],
                    evidence_text=evidence_text,
                )
            
            # ========== åˆ›æ–°æœºåˆ¶2: åº”ç”¨çº¢è“é˜Ÿç‰¹å®šæç¤º ==========
            if DEBATE_CONFIG["adversarial"]["enabled"]:
                prompt = get_team_specific_prompt(agent_team, base_prompt)
            else:
                prompt = base_prompt
            
            # è°ƒç”¨APIè·å–å›ç­”
            print("æ­£åœ¨è°ƒç”¨æ™ºè°±GLM-4.6 API...")
            response = call_glm_api(prompt, api_key, agent_info['role'])

            agents_responses[agent_idx].append(response)
            
            # è§£æç»“æ„åŒ–è¾“å‡ºå¹¶ä¿å­˜
            try:
                structured = parse_structured_response(response)
            except Exception:
                structured = {"summary": "", "confidence": 0.5, "raw": response}
            agents_structured[agent_idx].append(structured)
            
            # ========== åˆ›æ–°æœºåˆ¶4: è¯æ®é“¾éªŒè¯ ==========
            if DEBATE_CONFIG["evidence_chain"]["enabled"]:
                is_valid, missing, ev_quality = validate_evidence_chain(structured)
                if not is_valid:
                    print(f"âš ï¸ è¯æ®é“¾ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing)}")
                print(f"ğŸ“Š è¯æ®è´¨é‡åˆ†æ•°: {ev_quality:.2f}")
                
                # æå–è¯æ®å¼•ç”¨
                evidence_refs = extract_evidence_references(structured.get("evidence", ""))
                if evidence_refs:
                    print(f"ğŸ”— æ•°æ®å¼•ç”¨: {len(evidence_refs)}å¤„")
            
            print(f"\nğŸ“¢ {agent_name} çš„è¯„ä¼°:")
            print(f"{response}")
            print("-" * 60)
            
            # é¢å¤–æ‰“å°è§£æåçš„ç»“æ„åŒ–è¦ç‚¹ä¸è¯æ®
            try:
                print("ğŸ” ç»“æ„åŒ–è¦ç‚¹:")
                print(f"CLAIM: {structured.get('claim','')}")
                ev = structured.get('evidence','')
                if ev:
                    print("EVIDENCE:")
                    print(ev)
            except Exception:
                pass
            time.sleep(2)  # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
    
        # ========== æ¯è½®ç»“æŸåçš„åˆ†æ ==========
        try:
            current_structs = [agents_structured[i][round_idx] for i in range(agents)]
        except Exception:
            current_structs = []
        
        if current_structs:
            # æ›´æ–°æƒé‡
            agent_weights = update_agent_weights(agent_weights, current_structs, alpha=1.0, beta=1.0)
            weights_history.append(agent_weights)
            
            # ========== åˆ›æ–°æœºåˆ¶5: å…±è¯†å»ºæ¨¡ ==========
            if DEBATE_CONFIG["consensus_modeling"]["enabled"]:
                consensus = compute_multi_dimensional_consensus(current_structs, agent_names)
                consensus_history.append(consensus)
                print(f"\nğŸ“ˆ å¤šç»´åº¦å…±è¯†åˆ†æ (ç¬¬{round_idx+1}è½®):")
                print(f"  - åˆ†æ•°å…±è¯†: {consensus.get('score', 0):.2f}")
                print(f"  - è¯­ä¹‰ç›¸ä¼¼åº¦: {consensus.get('semantic', 0):.2f}")
                print(f"  - ä¼˜å…ˆçº§å¯¹é½: {consensus.get('priority', 0):.2f}")
                print(f"  - å…³æ³¨ç‚¹é‡å : {consensus.get('concern', 0):.2f}")
                print(f"  - æ•´ä½“å…±è¯†: {consensus.get('overall', 0):.2f}")
                
                # è¯†åˆ«åˆ†æ­§
                disagreements = identify_disagreement_types(current_structs)
                if any(disagreements.values()):
                    print(f"\nğŸ” è¯†åˆ«åˆ°çš„åˆ†æ­§:")
                    if disagreements["factual"]:
                        print(f"  - äº‹å®åˆ†æ­§: {len(disagreements['factual'])}å¤„")
                    if disagreements["interpretative"]:
                        print(f"  - è§£é‡Šåˆ†æ­§: {len(disagreements['interpretative'])}å¤„")
                    if disagreements["value_based"]:
                        print(f"  - ä»·å€¼åˆ†æ­§: {len(disagreements['value_based'])}å¤„")
            
            # ========== åˆ›æ–°æœºåˆ¶6: å…ƒè®¤çŸ¥è´¨é‡ç›‘æ§ ==========
            if DEBATE_CONFIG["meta_cognitive"]["enabled"]:
                quality = assess_debate_quality(current_structs, round_idx)
                quality_history.append(quality)
                print(f"\nğŸ“ è¾©è®ºè´¨é‡ç›‘æ§:")
                print(f"  - è¯æ®æä¾›ç‡: {quality['evidence']:.2f}")
                print(f"  - è§†è§’è¦†ç›–åº¦: {quality['coverage']:.2f}")
                print(f"  - é€»è¾‘è¿è´¯æ€§: {quality['coherence']:.2f}")
                print(f"  - æ–°é¢–åº¦: {quality['novelty']:.2f}")
                print(f"  - æ•´ä½“è´¨é‡: {quality['overall']:.2f}")
                
                # ç”Ÿæˆå¹²é¢„å»ºè®®
                if DEBATE_CONFIG["meta_cognitive"]["intervention_enabled"]:
                    interventions = generate_quality_intervention(quality)
                    if interventions:
                        print(f"\nğŸ’¡ è´¨é‡å¹²é¢„å»ºè®®:")
                        for interv in interventions:
                            print(f"  {interv}")
        
        # ========== æ—©åœåˆ¤æ®ï¼ˆå¢å¼ºç‰ˆï¼‰==========
        if round_idx > 0 and current_structs:
            prev_text = " ".join([agents_structured[i][round_idx-1].get('summary','') for i in range(agents)])
            curr_text = " ".join([s.get('summary','') for s in current_structs])
            sim = summary_similarity(prev_text, curr_text)
            
            # è€ƒè™‘å…±è¯†å’Œè´¨é‡çš„æ—©åœ
            should_early_stop = False
            if DEBATE_CONFIG["consensus_modeling"]["enabled"] and consensus:
                consensus_threshold = DEBATE_CONFIG["consensus_modeling"]["convergence_threshold"]
                if consensus.get("overall", 0) > consensus_threshold:
                    print(f"\nâ¹ï¸ æ—©åœ: å…±è¯†åº¦ {consensus['overall']:.2f} è¶…è¿‡é˜ˆå€¼ {consensus_threshold}")
                    should_early_stop = True
            
            if sim > 0.92:
                print(f"\nâ¹ï¸ æ—©åœ: ç›¸ä¼¼åº¦ {sim:.2f} è¶…è¿‡é˜ˆå€¼ 0.92")
                should_early_stop = True
            
            if should_early_stop:
                break

    # ç”Ÿæˆç»¼åˆè¯„åˆ†æŠ¥å‘Š
    if "mission_info" not in flight_data:
        flight_data["mission_info"] = {"duration_minutes": 45}
    scoring_report = generate_scoring_report(flight_data, scores, weighted_scores, expert_scorings)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_result = {
        "question": DEBATE_QUESTION,
        "flight_data": flight_data,
        "expert_roles": agent_names,
        "expert_evaluations": agents_responses,
        "structured_evaluations": agents_structured,
        "debate_weights_history": weights_history,
        "scoring_report": scoring_report,
        "model": "glm-4.6",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        # åˆ›æ–°æœºåˆ¶æ•°æ®
        "innovation_mechanisms": {
            "hierarchical_debate": {
                "complexity": complexity,
                "debate_layer": debate_layer,
                "planned_rounds": rounds,
                "actual_rounds": actual_rounds
            },
            "adversarial_collaborative": {
                "enabled": DEBATE_CONFIG["adversarial"]["enabled"],
                "team_history": team_assignments_history
            },
            "consensus_modeling": {
                "enabled": DEBATE_CONFIG["consensus_modeling"]["enabled"],
                "consensus_history": consensus_history
            },
            "meta_cognitive": {
                "enabled": DEBATE_CONFIG["meta_cognitive"]["enabled"],
                "quality_history": quality_history
            },
            "evidence_chain": {
                "enabled": DEBATE_CONFIG["evidence_chain"]["enabled"]
            },
            "role_rotation": {
                "enabled": DEBATE_CONFIG["role_rotation"]["enabled"],
                "mode": DEBATE_CONFIG["role_rotation"]["rotation_mode"]
            }
        }
    }
    
    with open("drone_swarm_evaluation_result.json", "w", encoding="utf-8") as f:
        json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ— äººæœºé›†ç¾¤è¯„ä¼°å®Œæˆï¼")
    print("ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° drone_swarm_evaluation_result.json")
    print("ğŸ“„ é£è¡Œæ•°æ®å·²ä¿å­˜åˆ° drone_flight_data.json")
    
    # æ˜¾ç¤ºè¯„åˆ†æŠ¥å‘Šæ‘˜è¦
    print(f"\nğŸ“Š ç»¼åˆè¯„åˆ†æŠ¥å‘Š:")
    print(f"  - æ€»ä½“è¯„åˆ†: {scoring_report['evaluation_summary']['total_score']} ({scoring_report['evaluation_summary']['grade']})")
    for category, data in scoring_report['category_scores'].items():
        print(f"  - {data['name']}: {data['score']} ({data['grade']})")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
    print(f"  - ä½¿ç”¨æ¨¡å‹: GLM-4.6")
    print(f"  - å‚ä¸ä¸“å®¶: {agents} ä½")
    print(f"  - è¯„ä¼°è½®æ¬¡: {rounds} è½®")
    print(f"  - æ€»APIè°ƒç”¨æ¬¡æ•°: {agents * rounds} æ¬¡")
    print(f"  - æ— äººæœºæ•°é‡: {flight_data['drone_count']} æ¶")
    print(f"  - é£è¡Œæ—¶é•¿: {flight_data['flight_duration']}")
    print(f"  - é›†ç¾¤è¡¨ç°: ç¨³å®šæ€§{flight_data['swarm_metrics']['formation_stability']}%, å®Œæˆç‡{flight_data['swarm_metrics']['task_completion_rate']}%")
    
    # æ‰“å°åˆ›æ–°æœºåˆ¶ä¿¡æ¯
    print(f"\nğŸ“Š åˆ›æ–°æœºåˆ¶ç»Ÿè®¡:")
    print(f"  - é—®é¢˜å¤æ‚åº¦: {complexity:.2f}")
    print(f"  - è·¯ç”±å±‚çº§: {debate_layer}")
    print(f"  - å®é™…è¾©è®ºè½®æ¬¡: {actual_rounds}")
    print(f"  - å…±è¯†å†å²è®°å½•: {len(consensus_history)} è½®")
    print(f"  - è´¨é‡ç›‘æ§è®°å½•: {len(quality_history)} è½®")
    if team_assignments_history:
        print(f"  - çº¢è“é˜Ÿè½®æ¢: {len(team_assignments_history)} æ¬¡")


if __name__ == "__main__":
        main()